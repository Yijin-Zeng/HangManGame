{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Hangman AI Strategy: Deep Learning Approach\n\nThis notebook demonstrates an AI strategy for playing Hangman using deep learning techniques.\nThe approach combines statistical analysis with LSTM neural networks to achieve high success rates.\n\n## ðŸŽ® What is Hangman?\n\nHangman is a classic word-guessing game where players try to discover a hidden word by guessing individual letters. Here's how it works:\n\n### Game Rules:\n1. **Setup**: A secret word is chosen and displayed as blank spaces (underscores), one for each letter\n2. **Guessing**: Players guess one letter at a time\n3. **Correct Guess**: If the letter appears in the word, all instances are revealed in their correct positions\n4. **Wrong Guess**: If the letter doesn't appear, it counts as a mistake\n5. **Win Condition**: Player wins by revealing the entire word before making too many wrong guesses\n6. **Lose Condition**: Player loses after 6 wrong guesses\n\n### Example Game:\n```\nSecret word: \"MACHINE\"\nInitial:     _ _ _ _ _ _ _\n\nGuess 'E': Correct!    â†’ _ _ _ _ _ _ E\nGuess 'A': Correct!    â†’ _ A _ _ _ _ E  \nGuess 'T': Wrong! (1/6) â†’ _ A _ _ _ _ E\nGuess 'I': Correct!    â†’ _ A _ _ I _ E\nGuess 'N': Correct!    â†’ _ A _ _ I N E\nGuess 'M': Correct!    â†’ M A _ _ I N E\nGuess 'C': Correct!    â†’ M A C _ I N E\nGuess 'H': Correct!    â†’ M A C H I N E  âœ… YOU WIN!\n```\n\n### The AI Challenge:\nThe strategic challenge is **letter selection** - which letter should you guess next to maximize your chances of:\n1. **Gaining information** about the word structure\n2. **Avoiding wrong guesses** that bring you closer to losing\n3. **Completing the word efficiently** with minimal guesses\n\nThis is where artificial intelligence can excel by learning patterns from thousands of words and games.\n\n## ðŸ“Š Project Overview\n- **Dataset**: 370K+ English words from public sources\n- **Strategy**: Multi-stage approach (early/mid/late game)\n- **Model**: Configurable LSTM architecture\n- **Evaluation**: Local simulation of 1000+ games\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Bidirectional, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## ðŸ§  Hangman AI Strategies\n\n### Traditional Approaches:\n1. **Frequency Analysis**: Guess letters based on how common they are in English (E, T, A, O, I, N...)\n2. **Pattern Matching**: Look for common word patterns and endings (-ING, -ED, -ION)\n3. **Dictionary Filtering**: Maintain a list of possible words and eliminate those that don't match\n\n### Our Deep Learning Approach:\nThis project implements a **multi-stage strategy** that adapts based on game state:\n\n- **Early Game**: When few letters are known, use statistical frequency\n- **Mid Game**: Combine frequency analysis with neural network predictions  \n- **Late Game**: Rely heavily on pattern recognition when word structure is clearer\n\nThe LSTM neural network learns to recognize word patterns and letter relationships from training on hundreds of thousands of English words, allowing it to make more intelligent guesses than pure frequency analysis.\"",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load public word dataset\n",
    "with open(\"words_public.txt\", \"r\") as f:\n",
    "    word_list = [line.strip().lower() for line in f if line.strip().isalpha() and len(line.strip()) >= 3]\n",
    "\n",
    "print(f\"Loaded {len(word_list)} words\")\n",
    "print(f\"Sample words: {word_list[:10]}\")\n",
    "print(f\"Word length range: {min(len(w) for w in word_list)} - {max(len(w) for w in word_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split words into training and testing sets\n",
    "train_words, test_words = train_test_split(word_list, test_size=0.2, random_state=42)\n",
    "print(f\"Training words: {len(train_words)}\")\n",
    "print(f\"Testing words: {len(test_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze word length distribution\n",
    "max_word_length = max(len(word) for word in train_words)\n",
    "word_lengths = [len(word) for word in train_words]\n",
    "word_length_counts = collections.Counter(word_lengths)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "lengths = list(word_length_counts.keys())\n",
    "counts = list(word_length_counts.values())\n",
    "plt.bar(lengths, counts, color='skyblue', alpha=0.7)\n",
    "plt.xlabel('Word Length')\n",
    "plt.ylabel('Number of Words')\n",
    "plt.title('Distribution of Word Lengths')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Letter frequency analysis\n",
    "plt.subplot(1, 2, 2)\n",
    "all_letters = ''.join(train_words)\n",
    "letter_freq = collections.Counter(all_letters)\n",
    "letters = string.ascii_lowercase\n",
    "frequencies = [letter_freq.get(letter, 0) for letter in letters]\n",
    "plt.bar(letters, frequencies, color='lightcoral', alpha=0.7)\n",
    "plt.xlabel('Letters')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Letter Frequency in Training Data')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Most common letters: {letter_freq.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configurable Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hangman_model(embedding_dim=128, lstm_layers=[512, 256, 128], \n",
    "                        dense_dim=64, dropout_rate=0.0, max_len=20):\n",
    "    \"\"\"\n",
    "    Create a configurable LSTM model for Hangman letter prediction.\n",
    "    \n",
    "    Args:\n",
    "        embedding_dim: Dimension of character embeddings\n",
    "        lstm_layers: List of LSTM layer sizes\n",
    "        dense_dim: Dense layer dimension\n",
    "        dropout_rate: Dropout rate (0.0 means no dropout)\n",
    "        max_len: Maximum sequence length\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    vocab_size = 28  # 0=Pad, 1-26=a-z, 27=_\n",
    "    \n",
    "    # Input layers\n",
    "    seq_input = Input(shape=(max_len,), name=\"seq_input\")\n",
    "    length_input = Input(shape=(1,), name=\"length_input\")\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, \n",
    "                         mask_zero=True)(seq_input)\n",
    "    \n",
    "    # LSTM layers\n",
    "    x = embedding\n",
    "    for i, units in enumerate(lstm_layers):\n",
    "        return_sequences = (i < len(lstm_layers) - 1)  # All but last layer return sequences\n",
    "        x = Bidirectional(LSTM(units, return_sequences=return_sequences))(x)\n",
    "        if dropout_rate > 0:\n",
    "            x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Combine with length information\n",
    "    combined = Concatenate()([x, length_input])\n",
    "    \n",
    "    # Dense layers\n",
    "    if dense_dim > 0:\n",
    "        combined = Dense(dense_dim, activation='relu')(combined)\n",
    "        if dropout_rate > 0:\n",
    "            combined = Dropout(dropout_rate)(combined)\n",
    "    \n",
    "    # Output layer\n",
    "    output = Dense(26, activation=\"sigmoid\", name=\"output\", kernel_regularizer=tf.keras.regularizers.l2(0.001))(combined)\n",
    "    \n",
    "    # Build and compile model\n",
    "    model = Model(inputs=[seq_input, length_input], outputs=output)\n",
    "    optimizer = Adam(learning_rate=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test different model configurations\n",
    "model_configs = {\n",
    "    'small': {'embedding_dim': 64, 'lstm_layers': [128, 64], 'dense_dim': 32},\n",
    "    'medium': {'embedding_dim': 128, 'lstm_layers': [256, 128], 'dense_dim': 64},\n",
    "    'large': {'embedding_dim': 128, 'lstm_layers': [512, 256, 128], 'dense_dim': 64},\n",
    "    'xlarge': {'embedding_dim': 256, 'lstm_layers': [512, 256, 128, 64], 'dense_dim': 128}\n",
    "}\n",
    "\n",
    "# Create a sample model to show architecture\n",
    "sample_model = create_hangman_model(**model_configs['medium'])\n",
    "sample_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary setup\n",
    "alphabet = string.ascii_lowercase\n",
    "vocab = ['Pad'] + list(alphabet) + ['_']\n",
    "char2idx = {ch: i for i, ch in enumerate(vocab)}\n",
    "letter_to_idx = {letter: idx for idx, letter in enumerate(alphabet)}\n",
    "\n",
    "def make_masked_word(word, guessed):\n",
    "    \"\"\"Create masked word with underscores for unguessed letters\"\"\"\n",
    "    return ''.join([c if c in guessed else '_' for c in word])\n",
    "\n",
    "def naive_method(word, max_word_length=20):\n",
    "    \"\"\"Get first few letters using frequency analysis\"\"\"\n",
    "    wrong_guess = []\n",
    "    correct_guess = []\n",
    "    word_set = set(word)\n",
    "    \n",
    "    if len(word_set) <= 2:\n",
    "        return [], list(word_set), []\n",
    "    \n",
    "    # Use simple frequency for first guesses\n",
    "    common_letters = ['e', 'a', 'i', 'o', 'u', 'r', 's', 't', 'l', 'n']\n",
    "    \n",
    "    for letter in common_letters:\n",
    "        if len(correct_guess) >= 2:\n",
    "            break\n",
    "        if letter in word_set:\n",
    "            correct_guess.append(letter)\n",
    "            word_set.remove(letter)\n",
    "        else:\n",
    "            wrong_guess.append(letter)\n",
    "    \n",
    "    return list(word_set), correct_guess, wrong_guess\n",
    "\n",
    "def create_training_examples(word, max_len=20):\n",
    "    \"\"\"Generate training samples for a given word\"\"\"\n",
    "    if len(set(word)) <= 2:\n",
    "        return []\n",
    "        \n",
    "    examples = []\n",
    "    unique_letters, correct_letters, _ = naive_method(word)\n",
    "    n = len(unique_letters)\n",
    "    \n",
    "    if n == 0:\n",
    "        return []\n",
    "    \n",
    "    # Generate combinations of revealed letters\n",
    "    for k in range(n):\n",
    "        combinations_list = list(combinations(unique_letters, k))\n",
    "        # Sample combinations to avoid too many examples\n",
    "        sample_size = min(len(combinations_list), max(1, k))\n",
    "        selected_combinations = random.sample(combinations_list, sample_size)\n",
    "        \n",
    "        for comb in selected_combinations:\n",
    "            masked = make_masked_word(word, list(comb) + correct_letters)\n",
    "            remaining_letters = [c for c in unique_letters if c not in comb]\n",
    "            guessed = correct_letters + list(comb)\n",
    "            \n",
    "            examples.append({\n",
    "                \"masked_word\": masked,\n",
    "                \"guessed_letters\": ''.join(guessed),\n",
    "                \"target\": ''.join(remaining_letters),\n",
    "                \"word_length\": len(word)\n",
    "            })\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "def generate_training_data(word_list, num_samples=50000, max_len=20):\n",
    "    \"\"\"Generate training examples from word list\"\"\"\n",
    "    all_examples = []\n",
    "    \n",
    "    # Sample words to control dataset size\n",
    "    if len(word_list) > num_samples:\n",
    "        selected_words = random.sample(word_list, num_samples)\n",
    "    else:\n",
    "        selected_words = word_list\n",
    "    \n",
    "    for word in selected_words:\n",
    "        if len(word) <= max_len and word.isalpha():\n",
    "            examples = create_training_examples(word.lower().strip(), max_len)\n",
    "            all_examples.extend(examples)\n",
    "    \n",
    "    print(f\"Generated {len(all_examples)} training examples from {len(selected_words)} words\")\n",
    "    return all_examples\n",
    "\n",
    "# Generate training data\n",
    "training_examples = generate_training_data(train_words, num_samples=20000)\n",
    "random.shuffle(training_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding functions\n",
    "def encode_input(masked_word, max_len=20):\n",
    "    \"\"\"Encode masked word to indices\"\"\"\n",
    "    input_ids = [char2idx[c] for c in masked_word]\n",
    "    if len(input_ids) < max_len:\n",
    "        input_ids += [char2idx['Pad']] * (max_len - len(input_ids))\n",
    "    else:\n",
    "        input_ids = input_ids[:max_len]\n",
    "    return input_ids\n",
    "\n",
    "def encode_target(target):\n",
    "    \"\"\"Encode target letters to binary vector\"\"\"\n",
    "    target_vec = [0] * 26\n",
    "    for c in target:\n",
    "        if c in string.ascii_lowercase:\n",
    "            target_vec[ord(c) - ord('a')] = 1\n",
    "    return target_vec\n",
    "\n",
    "def encode_guessed(guessed_letters):\n",
    "    \"\"\"Encode guessed letters to binary vector\"\"\"\n",
    "    guessed_vec = [0] * 26\n",
    "    for c in guessed_letters:\n",
    "        if c in string.ascii_lowercase:\n",
    "            guessed_vec[ord(c) - ord('a')] = 1\n",
    "    return guessed_vec\n",
    "\n",
    "# Prepare training data\n",
    "X_words = []\n",
    "X_lengths = []\n",
    "X_guessed = []\n",
    "y = []\n",
    "\n",
    "for example in training_examples:\n",
    "    word_encoded = encode_input(example['masked_word'])\n",
    "    length = example['word_length']\n",
    "    guessed_encoded = encode_guessed(example['guessed_letters'])\n",
    "    target_encoded = encode_target(example['target'])\n",
    "    \n",
    "    X_words.append(word_encoded)\n",
    "    X_lengths.append(length)\n",
    "    X_guessed.append(guessed_encoded)\n",
    "    y.append(target_encoded)\n",
    "\n",
    "X_words = np.array(X_words)\n",
    "X_lengths = np.array(X_lengths).reshape(-1, 1)\n",
    "X_guessed = np.array(X_guessed)\n",
    "y = np.array(y)\n",
    "\n",
    "print(f\"Training data shapes:\")\n",
    "print(f\"Words: {X_words.shape}\")\n",
    "print(f\"Lengths: {X_lengths.shape}\")\n",
    "print(f\"Targets: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data\n",
    "X_train_words, X_val_words, X_train_lengths, X_val_lengths, y_train, y_val = train_test_split(\n",
    "    X_words, X_lengths, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "def train_model(config_name, config, epochs=20, batch_size=512):\n",
    "    \"\"\"Train a model with given configuration\"\"\"\n",
    "    print(f\"\\nTraining {config_name} model...\")\n",
    "    \n",
    "    model = create_hangman_model(**config)\n",
    "    \n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        [X_train_words, X_train_lengths], y_train,\n",
    "        validation_data=([X_val_words, X_val_lengths], y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    model.save(f\"hangman_model_{config_name}.keras\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Train different model sizes\n",
    "trained_models = {}\n",
    "training_histories = {}\n",
    "\n",
    "for config_name, config in model_configs.items():\n",
    "    model, history = train_model(config_name, config)\n",
    "    trained_models[config_name] = model\n",
    "    training_histories[config_name] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, (config_name, history) in enumerate(training_histories.items()):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{config_name.capitalize()} Model Training')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Local Hangman Game Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HangmanGame:\n",
    "    \"\"\"Local Hangman game simulator\"\"\"\n",
    "    \n",
    "    def __init__(self, model, word_list, max_len=20):\n",
    "        self.model = model\n",
    "        self.word_list = word_list\n",
    "        self.max_len = max_len\n",
    "        self.vocab = ['Pad'] + list(string.ascii_lowercase) + ['_']\n",
    "        self.char2idx = {ch: i for i, ch in enumerate(self.vocab)}\n",
    "        self.letter_to_idx = {letter: idx for idx, letter in enumerate(string.ascii_lowercase)}\n",
    "        \n",
    "    def encode_word(self, masked_word):\n",
    "        \"\"\"Encode masked word for model input\"\"\"\n",
    "        input_ids = [self.char2idx[c] for c in masked_word]\n",
    "        if len(input_ids) < self.max_len:\n",
    "            input_ids += [self.char2idx['Pad']] * (self.max_len - len(input_ids))\n",
    "        else:\n",
    "            input_ids = input_ids[:self.max_len]\n",
    "        return input_ids\n",
    "    \n",
    "    def encode_guessed_letters(self, guessed_letters):\n",
    "        \"\"\"Encode guessed letters as binary vector\"\"\"\n",
    "        target_vec = [0] * 26\n",
    "        for c in guessed_letters:\n",
    "            if c in string.ascii_lowercase:\n",
    "                target_vec[ord(c) - ord('a')] = 1\n",
    "        return target_vec\n",
    "    \n",
    "    def prob_screening(self, guessed_wrong_letters, guessed_correct_letters, word_length):\n",
    "        \"\"\"Statistical frequency analysis for early game\"\"\"\n",
    "        guessed_wrong_set = set(guessed_wrong_letters)\n",
    "        guessed_correct_letters = set(guessed_correct_letters)\n",
    "        \n",
    "        # Filter words\n",
    "        possible_words = [\n",
    "            word for word in self.word_list\n",
    "            if len(word) == word_length and \n",
    "            guessed_wrong_set.isdisjoint(word) and \n",
    "            guessed_correct_letters.issubset(word)\n",
    "        ]\n",
    "        \n",
    "        # Calculate letter probabilities\n",
    "        prob = [0] * 26\n",
    "        for word in possible_words:\n",
    "            for letter in set(word):\n",
    "                prob[self.letter_to_idx[letter]] += 1\n",
    "        \n",
    "        n = max(len(possible_words), 1)\n",
    "        epsilon = 0.0001\n",
    "        prob = [p/n + epsilon for p in prob]\n",
    "        \n",
    "        return len(possible_words), np.array(prob)\n",
    "    \n",
    "    def guess(self, word, tries_remaining, guessed_letters):\n",
    "        \"\"\"Make a guess using the three-stage strategy\"\"\"\n",
    "        word = word.replace(\" \", \").lower()\n",
    "        correct_letters = [c for c in set(word) if c != '_']\n",
    "        wrong_letters = [c for c in guessed_letters if c not in correct_letters]\n",
    "        word_len = min(len(word), self.max_len)\n",
    "        \n",
    "        known_number = len(set(word)) - 1  # minus '_'\n",
    "        \n",
    "        if known_number < 2:\n",
    "            # Early stage: use frequency analysis\n",
    "            _, prob_vector = self.prob_screening(wrong_letters, correct_letters, word_len)\n",
    "            sorted_indices = np.argsort(prob_vector)[::-1]\n",
    "            for idx in sorted_indices:\n",
    "                letter = chr(ord('a') + idx)\n",
    "                if letter not in guessed_letters:\n",
    "                    return letter\n",
    "        else:\n",
    "            # Mid/Late stage: use model prediction\n",
    "            len_dict, uncon_prob = self.prob_screening(wrong_letters, correct_letters, word_len)\n",
    "            \n",
    "            # Encode for model\n",
    "            word_encoded = np.array(self.encode_word(word)).reshape(1, -1)\n",
    "            word_len_encoded = np.array(word_len).reshape(1, -1)\n",
    "            \n",
    "            # Get model prediction\n",
    "            probs = self.model.predict([word_encoded, word_len_encoded], verbose=0)[0]\n",
    "            \n",
    "            # Combine with frequency if many candidate words remain\n",
    "            if len_dict > 200 and tries_remaining > 2:\n",
    "                probs = uncon_prob * probs\n",
    "            \n",
    "            # Mask already guessed letters\n",
    "            guessed_mask = self.encode_guessed_letters(guessed_letters)\n",
    "            probs[np.array(guessed_mask, dtype=bool)] = -1\n",
    "            \n",
    "            # Select best letter\n",
    "            next_letter_index = np.argmax(probs)\n",
    "            return string.ascii_lowercase[next_letter_index]\n",
    "        \n",
    "        return 'e'  # fallback\n",
    "    \n",
    "    def play_game(self, word, verbose=False):\n",
    "        \"\"\"Play a complete game of Hangman\"\"\"\n",
    "        word = word.lower()\n",
    "        tries_remaining = 6\n",
    "        guessed_letters = []\n",
    "        current_state = '_' * len(word)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Playing word: {word} (length: {len(word)})\")\n",
    "        \n",
    "        while tries_remaining > 0 and '_' in current_state:\n",
    "            guess = self.guess(current_state, tries_remaining, guessed_letters)\n",
    "            guessed_letters.append(guess)\n",
    "            \n",
    "            if guess in word:\n",
    "                # Update current state\n",
    "                new_state = ''.join([c if c in guessed_letters else '_' for c in word])\n",
    "                current_state = new_state\n",
    "                if verbose:\n",
    "                    print(f\"Guess '{guess}': HIT! Current: {current_state}\")\n",
    "            else:\n",
    "                tries_remaining -= 1\n",
    "                if verbose:\n",
    "                    print(f\"Guess '{guess}': MISS! Tries left: {tries_remaining}\")\n",
    "        \n",
    "        success = '_' not in current_state\n",
    "        if verbose:\n",
    "            print(f\"Game {'WON' if success else 'LOST'}! Final word: {word}\")\n",
    "            print()\n",
    "        \n",
    "        return success, len(word), 6 - tries_remaining\n",
    "    \n",
    "    def simulate_games(self, num_games=1000, verbose_interval=100):\n",
    "        \"\"\"Simulate multiple games and return statistics\"\"\"\n",
    "        results = []\n",
    "        test_words_sample = random.sample(self.word_list, min(num_games, len(self.word_list)))\n",
    "        \n",
    "        for i, word in enumerate(test_words_sample):\n",
    "            success, word_length, guesses_used = self.play_game(word, verbose=(i % verbose_interval == 0))\n",
    "            results.append({\n",
    "                'word': word,\n",
    "                'success': success,\n",
    "                'length': word_length,\n",
    "                'guesses_used': guesses_used\n",
    "            })\n",
    "            \n",
    "            if (i + 1) % verbose_interval == 0:\n",
    "                current_success_rate = sum(r['success'] for r in results) / len(results)\n",
    "                print(f\"Completed {i+1}/{num_games} games. Success rate: {current_success_rate:.3f}\")\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all models\n",
    "model_results = {}\n",
    "num_test_games = 500  # Adjust based on computational resources\n",
    "\n",
    "for config_name, model in trained_models.items():\n",
    "    print(f\"\\nTesting {config_name} model...\")\n",
    "    game = HangmanGame(model, test_words)\n",
    "    results = game.simulate_games(num_test_games, verbose_interval=100)\n",
    "    \n",
    "    success_rate = sum(r['success'] for r in results) / len(results)\n",
    "    avg_guesses = np.mean([r['guesses_used'] for r in results])\n",
    "    \n",
    "    model_results[config_name] = {\n",
    "        'success_rate': success_rate,\n",
    "        'avg_guesses': avg_guesses,\n",
    "        'results': results\n",
    "    }\n",
    "    \n",
    "    print(f\"{config_name} model: {success_rate:.3f} success rate, {avg_guesses:.1f} avg guesses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Overall performance comparison\n",
    "plt.subplot(2, 3, 1)\n",
    "models = list(model_results.keys())\n",
    "success_rates = [model_results[m]['success_rate'] for m in models]\n",
    "plt.bar(models, success_rates, color='skyblue', alpha=0.7)\n",
    "plt.title('Success Rate by Model Size')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Average guesses comparison\n",
    "plt.subplot(2, 3, 2)\n",
    "avg_guesses = [model_results[m]['avg_guesses'] for m in models]\n",
    "plt.bar(models, avg_guesses, color='lightcoral', alpha=0.7)\n",
    "plt.title('Average Guesses Used by Model Size')\n",
    "plt.ylabel('Average Guesses')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Performance by word length (using best model)\n",
    "best_model = max(models, key=lambda m: model_results[m]['success_rate'])\n",
    "best_results = model_results[best_model]['results']\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "length_success = {}\n",
    "for result in best_results:\n",
    "    length = result['length']\n",
    "    if length not in length_success:\n",
    "        length_success[length] = []\n",
    "    length_success[length].append(result['success'])\n",
    "\n",
    "lengths = sorted(length_success.keys())\n",
    "success_by_length = [np.mean(length_success[l]) for l in lengths]\n",
    "plt.plot(lengths, success_by_length, 'o-', color='green', linewidth=2, markersize=6)\n",
    "plt.title(f'Success Rate by Word Length\\n({best_model} model)')\n",
    "plt.xlabel('Word Length')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Model parameters vs performance\n",
    "plt.subplot(2, 3, 4)\n",
    "model_params = []\n",
    "for config_name in models:\n",
    "    model = trained_models[config_name]\n",
    "    params = model.count_params()\n",
    "    model_params.append(params)\n",
    "\n",
    "plt.scatter(model_params, success_rates, s=100, alpha=0.7, color='purple')\n",
    "for i, config_name in enumerate(models):\n",
    "    plt.annotate(config_name, (model_params[i], success_rates[i]), \n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "plt.title('Model Size vs Performance')\n",
    "plt.xlabel('Number of Parameters')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Success rate distribution\n",
    "plt.subplot(2, 3, 5)\n",
    "all_success_rates = []\n",
    "labels = []\n",
    "for config_name in models:\n",
    "    successes = [r['success'] for r in model_results[config_name]['results']]\n",
    "    all_success_rates.append(successes)\n",
    "    labels.append(config_name)\n",
    "\n",
    "plt.boxplot(all_success_rates, labels=labels)\n",
    "plt.title('Success Rate Distribution')\n",
    "plt.ylabel('Success (0=Loss, 1=Win)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Summary statistics\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.axis('off')\n",
    "summary_text = \"Model Performance Summary\\n\\n\"\n",
    "for config_name in models:\n",
    "    sr = model_results[config_name]['success_rate']\n",
    "    ag = model_results[config_name]['avg_guesses']\n",
    "    params = trained_models[config_name].count_params()\n",
    "    summary_text += f\"{config_name}: {sr:.3f} ({ag:.1f} guesses, {params:,} params)\\n\"\n",
    "\n",
    "plt.text(0.1, 0.9, summary_text, transform=plt.gca().transAxes, \n",
    "         fontsize=10, verticalalignment='top', fontfamily='monospace')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of best performing model\n",
    "best_model_name = max(model_results.keys(), key=lambda m: model_results[m]['success_rate'])\n",
    "best_results = model_results[best_model_name]['results']\n",
    "\n",
    "print(f\"\\nDetailed Analysis of {best_model_name} Model:\")\n",
    "print(f\"Overall Success Rate: {model_results[best_model_name]['success_rate']:.3f}\")\n",
    "print(f\"Average Guesses Used: {model_results[best_model_name]['avg_guesses']:.1f}\")\n",
    "\n",
    "# Success rate by word length\n",
    "length_analysis = {}\n",
    "for result in best_results:\n",
    "    length = result['length']\n",
    "    if length not in length_analysis:\n",
    "        length_analysis[length] = {'wins': 0, 'total': 0, 'guesses': []}\n",
    "    \n",
    "    length_analysis[length]['total'] += 1\n",
    "    length_analysis[length]['guesses'].append(result['guesses_used'])\n",
    "    if result['success']:\n",
    "        length_analysis[length]['wins'] += 1\n",
    "\n",
    "print(\"\\nPerformance by Word Length:\")\n",
    "for length in sorted(length_analysis.keys()):\n",
    "    data = length_analysis[length]\n",
    "    if data['total'] >= 5:  # Only show lengths with sufficient data\n",
    "        success_rate = data['wins'] / data['total']\n",
    "        avg_guesses = np.mean(data['guesses'])\n",
    "        print(f\"Length {length:2d}: {success_rate:.3f} success rate ({data['wins']:2d}/{data['total']:2d} games), {avg_guesses:.1f} avg guesses\")\n",
    "\n",
    "# Model comparison summary\n",
    "print(\"\\nModel Comparison Summary:\")\n",
    "for config_name in ['small', 'medium', 'large', 'xlarge']:\n",
    "    if config_name in model_results:\n",
    "        sr = model_results[config_name]['success_rate']\n",
    "        params = trained_models[config_name].count_params()\n",
    "        print(f\"{config_name:6s}: {sr:.3f} success rate, {params:7,d} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Strategy Summary\n",
    "\n",
    "This notebook demonstrates a sophisticated AI approach to playing Hangman with the following key components:\n",
    "\n",
    "### Multi-Stage Strategy\n",
    "1. **Early Stage** (0-1 letters known): Statistical frequency analysis\n",
    "2. **Mid Stage** (2+ letters, many candidates): Hybrid frequency + neural network\n",
    "3. **Late Stage** (few candidates or critical decisions): Pure neural network pattern recognition\n",
    "\n",
    "### Key Findings\n",
    "- Larger models generally perform better, but with diminishing returns\n",
    "- Performance strongly correlates with word length (better on longer words)\n",
    "- The three-stage strategy effectively adapts to different game phases\n",
    "- LSTM networks excel at pattern completion when sufficient context is available\n",
    "\n",
    "### Future Improvements\n",
    "- Specialized models for short words (â‰¤7 letters)\n",
    "- Adaptive stage transitions based on word length\n",
    "- Integration of word frequency information\n",
    "- Ensemble methods combining multiple model sizes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}